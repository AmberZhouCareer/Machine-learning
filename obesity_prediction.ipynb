{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Obesity Diagnosis based on eating Habits and physical condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name:Yuqing Zhou\n",
    "\n",
    "Student ID:1140794"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Due date</b>: Monday, 6 September 2021 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day (both week and weekend days counted up to 5 days)\n",
    "<ul>\n",
    "    <li>one day late, -1.5;</li>\n",
    "    <li>two days late, -3.0;</li>\n",
    "    <li>three days late, -4.5;</li>\n",
    "    <li>four days late, -6.0;</li>\n",
    "    <li>five days late, -7.5;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: 15% of mark for class. \n",
    "\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/105477/pages/python-and-jupyter-notebooks?module_item_id=2613813) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages Numpy, Scipy, Scikit-Learn. The instructions for each question will state what libraries you can use; if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. While the main focus is on correctness of your methods, you will lose marks if your code is not understandable.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourage you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will be working with *obesity* dataset containing data for the estimation of obesity levels in individuals from the countries of Mexico, Peru and Colombia, based on their eating habits and physical condition. The data contains 16 attributes describing 2111 patients. The dataset provided for this assignment is derived from <a href=\"https://archive.ics.uci.edu/ml/datasets/Estimation+of+obesity+levels+based+on+eating+habits+and+physical+condition+#\">this</a> resource. Your task is to predict whether a patient is considered obese (mass body index >=30) or not (mass body index <30).\n",
    "\n",
    "The attributes are\n",
    "\n",
    "|ID|Feature Name| Feature Type | Feature Values|\n",
    "| :-| :-| :-| :-|\n",
    "|0|Gender| categorical | Female: 0, Male: 1|\n",
    "|1|Age| continuous | |\n",
    "|2|Height| continuous | |\n",
    "|3|Weight| continuous | ? |\n",
    "|4|family_history| categorical | yes: 1, no: 0|\n",
    "|5|FACV| categorical | yes: 1, no: 0|\n",
    "|6|FCVC| continuous | |\n",
    "|7|NCP| continuous | |\n",
    "|8|CAEC| categorical | Always: 0, Frequently: 1, Sometimes: 2, no:  3|\n",
    "|9|SMOKE| categorical | yes: 1, no: 0|\n",
    "|10|CH2O| continuous | |\n",
    "|11|SCC| categorical | yes: 1, no: 0|\n",
    "|12|FAF| continuous | |\n",
    "|13|TUE| continuous | |\n",
    "|14|CALC| categorical | Always: 0, Frequently: 1, Sometimes: 2, no:  3, ?|\n",
    "|15|MTRANS| categorical | Automobile: 0, Bike: 1, Motorbike: 2, Public_Transportation: 3, Walking: 4, ?|\n",
    "\n",
    "You can find out more about the individual attributes / values, and the origin of the data set in this <a href=\"https://www.sciencedirect.com/science/article/pii/S2352340919306985\"> article</a>  \n",
    "\n",
    "\n",
    "You will build a number of classifiers to predict whether a patient is obese or not based on the attributes above.\n",
    "\n",
    "\n",
    "#### The following instructions hold for every question in the assignment\n",
    "- leave the order of instances intact, i.e., do not shuffle the data\n",
    "- do not change the names or types of variables provided by us in the code cells below\n",
    "- '?' denotes a missing value, and you should treat it as instructed in Question 1-B.\n",
    "- the mapping of categorical feature values to integers indicates how you should process your data in 1-C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Loading and pre-processing the data [2 marks]\n",
    "\n",
    "You were provided with two data files:\n",
    "\n",
    "**obesity.train** contains about 1,688 training instances, one instance per line in comma-separated value (csv) format. Each line contains 17 fields. The first 16 fields correspond to the features listed above, the final field denotes the class label\n",
    "\n",
    "**obesity.test** is formatted exactly like obesity.train, and contains about 423 further instances for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-A Read the data [0.25 marks]\n",
    "\n",
    "First, you will read in the data and create train features, train labels, test features and test labels. Do not apply any data transformations in this step and keep the '?' as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 1688, 423, 423, 16, 16)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data = open(\"obesity.train\",'r').readlines()\n",
    "test_data = open(\"obesity.test\",'r').readlines()\n",
    "feature_names = [\"gender\",\"age\",\"height\",\"weight\",\"family_history\",\"FACV\",\"FCVC\",\"NCP\",\"CAEC\",\"smoke\",\"CH2O\",\"SCC\",\"FAF\",\"TUE\",\"CALC\",\"MTRANS\"]\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "for row in data:\n",
    "    row = row.replace(\"\\n\",\"\")\n",
    "    row = row.split(',')\n",
    "    feature_train = []\n",
    "    \n",
    "    for num in row[:16]:\n",
    "        feature_train.append(num)\n",
    "    x_train.append(feature_train)\n",
    "    y_train.append(row[16])\n",
    "\n",
    "        \n",
    "for row in test_data:\n",
    "    row = row.replace(\"\\n\",\"\")\n",
    "    row = row.split(',')\n",
    "    feature_test = []\n",
    "\n",
    "    for num in row[:16]:\n",
    "        feature_test.append(num)\n",
    "    x_test.append(feature_test)\n",
    "    y_test.append(row[16])\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "len(x_train), len(y_train), len(x_test), len(y_test), len(x_train[0]), len(x_test[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train)==len(y_train)==1688\n",
    "assert len(x_train[0])==len(x_test[0])==16\n",
    "assert len(x_test)==len(y_test)==423\n",
    "assert x_train[5][2] == '1.7544389999999999'\n",
    "assert x_test[8][6] == '2.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-B: Dealing with Missing Values [0.75 marks]\n",
    "\n",
    "You will write <b>two functions</b> to replace missing values using the strategies below based on the different attribute types (categorical, continuous):\n",
    "\n",
    "   * <b>Continuous features</b>: For each feature find the <b>average feature value</b> in the train dataset \n",
    "   * <b>Categorical features</b>: For each feature find the <b>most frequent value</b> in the train dataset \n",
    "\n",
    "and a <b>third function</b> that replaces the missing values in the train and test datasets with the most frequent (categorical features) and average values (continuous features) that you found in the train dataset.\n",
    " \n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "You will implement three functions\n",
    "1. `num_mean_feat`\n",
    "<ul>\n",
    "  <li>input:  instances with only continuous features</li>\n",
    "  <li>output: a list containing the average value for each continuous feature (aka substitute values)</li>\n",
    "</ul> \n",
    "    \n",
    "2. `cat_most_feat`\n",
    "<ul>\n",
    "    <li>input: instances with only categorical features</li>\n",
    "    <li>output: a list containing the most frequent feature value for each categorical feature (aka substitute values)</li>\n",
    "</ul>\n",
    "\n",
    "3. `transform` \n",
    "<ul>\n",
    "    <li>input:\n",
    "        <ul>\n",
    "            <li>a list of instances</li>   \n",
    "            <li>replacement list containing the substitute values for each feature</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>output: the transformed instances</li>\n",
    "</ul>\n",
    "\n",
    "<b> You should implement these functions from scratch yourself. You may use native Python libraries such as math, collections, or numpy to help you, but you may not use existing implementations.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "def num_mean_fit(X):\n",
    "    replacement = []\n",
    "    ## insert your code here    \n",
    "    import numpy as np\n",
    "    feature = np.array(X).T\n",
    "    for row in feature:\n",
    "        n = 0\n",
    "        total = 0\n",
    "        for num in row:\n",
    "            if num != \"?\":\n",
    "                n = n+1\n",
    "                total = total + float(num)\n",
    "        feature_x = total/n\n",
    "        replacement.append(feature_x)\n",
    "    return replacement\n",
    "\n",
    "def cat_most_fit(X):\n",
    "    replacement = []\n",
    "    \n",
    "    ## insert your code here\n",
    "    import numpy as np\n",
    "    feature = np.array(X).T\n",
    "    for row in feature:\n",
    "        feature_x = max(list(row),key=list(row).count)\n",
    "        replacement.append(feature_x)\n",
    "    return replacement \n",
    "\n",
    "def transform(X, replacement):\n",
    "    trans_X  = copy.deepcopy(X)       \n",
    "       \n",
    "    ## insert your code here\n",
    "    for i in range(len(trans_X)):\n",
    "        for j in range(len(trans_X[i])):\n",
    "            if trans_X[i][j] ==\"?\":\n",
    "                trans_X[i][j] = replacement[j]\n",
    "    return trans_X\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert cat_most_fit([['no','always'],['yes','always'],['yes','?']]) == ['yes','always']\n",
    "assert num_mean_fit([[12,122,'?'],[21,150,78],[18,'?',79]]) == [17,136,78.5]\n",
    "assert transform([[10,144,'?'],[92,'?',70]], [17,136,78.5]) == [[10, 144, 78.5], [92, 136, 70]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-C: Attribute Types [1 marks]\n",
    "\n",
    "You will create four feature representations, based on the different attribute types (categorical, continuous) in the original *Obesity* data.\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "Denote with $I$ the number of training instances; $C$ the number of categorical features in the data set; $N$ the number of continuous features; $v_{f}$ the number of values for categorical feature $f$; and $v_{bin}$ the number of bins for continuous features. \n",
    "\n",
    "1. Create a train data set with only numeric features `x_train_num` (size $(I\\times N)$); and equivalently a test data set `x_test_num`; the missing values in train and test must be replaced with the average feature value found in the train dataset\n",
    "\n",
    "2. Create a train data set with only categorical features `x_train_cat` (size $(I\\times C)$); and equivalently a test data set `x_test_cat`; the missing values in train and test must be replaced with the most frequent feature values in the train dataset; You need to convert the feature values in `x_train_cat` and `x_test_cat` to integers (as shown in the Overview table) to be able to further use them in the rest of your program\n",
    "\n",
    "3. Create a train data set with both continuous `x_train_num` and a 1-hot representation of categorical features `x_train_1hot` and save into a variable called `x_train_num_full` (size $I \\times (\\sum_f v_{f}+N)$) where the first $\\sum_f v_f$ columns represent the 1-hot encoded categorical features and the remaining columns the continuous features; and equivalently a test data set `x_test_num_full`\n",
    "\n",
    "4. Create a train data set using both categorical features `x_train_cat` and discretized numeric features `x_train_disc`(use equal-width binning with 4 bins, $v_{bin}=4$) and save into a variable called `x_train_cat_full` (size $I\\times (C+N)$); and equivalently a test data set `x_test_cat_full`; You need to convert the feature values in `x_train_cat_full` and `x_test_cat_full` to integers (as shown in the Overview table) to be able to further use them in the rest of your program.\n",
    "\n",
    "\n",
    "You will create two lists of feature names for the full categorical and full continuous datasets.\n",
    "\n",
    "1. Create a list of feature names (string) for the `x_{train,test}_num_full` dataset (size $C+N$).\n",
    "\n",
    "2. Create a list of feature names (string) for the `x_{train,test}_cat_full` dataset (size $\\sum_f v_{f}+N$).\n",
    "\n",
    "\n",
    "**Note:** You may use classes and functions from ```scikit-learn```. You may use native Python libraries such as math, collections, or numpy to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num = []\n",
    "x_test_num = []\n",
    "\n",
    "x_train_cat = []\n",
    "x_test_cat = []\n",
    "\n",
    "x_train_num_full = []\n",
    "x_test_num_full = []\n",
    "\n",
    "x_train_cat_full = []\n",
    "x_test_cat_full = []\n",
    "\n",
    "full_num_feature_names = []\n",
    "full_cat_feature_names = []\n",
    "\n",
    "################################\n",
    "### Your code begins here ######\n",
    "################################\n",
    "import numpy as np\n",
    "feature = np.array(x_train).T\n",
    "# Extract continuous features from test Instances\n",
    "num_id = [1,2,3,6,7,10,12,13]\n",
    "for i in num_id:\n",
    "    x_train_num.append(feature[i])\n",
    "x_train_num = np.array(x_train_num).T.tolist()\n",
    "replacement_num = num_mean_fit(x_train_num)\n",
    "x_train_num = transform(x_train_num, replacement_num)\n",
    "num_features_name = []\n",
    "for i in num_id:\n",
    "    num_features_name.append(feature_names[i])\n",
    "\n",
    "# Handle continuous Features exception data in test instance\n",
    "feature = np.array(x_test).T\n",
    "for i in num_id:\n",
    "    x_test_num.append(feature[i])\n",
    "x_test_num = np.array(x_test_num).T.tolist()\n",
    "x_test_num = transform(x_test_num, replacement_num)\n",
    "\n",
    "# Extract categorical features from test Instances\n",
    "feature = np.array(x_train).T\n",
    "cat_id = [0,4,5,8,9,11,14,15]\n",
    "for i in cat_id:\n",
    "    x_train_cat.append(feature[i])\n",
    "x_train_cat = np.array(x_train_cat).T.tolist()\n",
    "replacement_cat = cat_most_fit(x_train_cat)\n",
    "x_train_cat = transform(x_train_cat, replacement_cat)\n",
    "cat_features_name = []\n",
    "for i in cat_id:\n",
    "    cat_features_name.append(feature_names[i])\n",
    "    \n",
    "# Handle categorical Features exception data in test instance\n",
    "feature = np.array(x_test).T\n",
    "for i in cat_id:\n",
    "    x_test_cat.append(feature[i])\n",
    "x_test_cat = np.array(x_test_cat).T.tolist()\n",
    "x_test_cat = transform(x_test_cat, replacement_cat)\n",
    "\n",
    "# create two lists of feature names\n",
    "replacement_rule1 ={\"Female\":0, \"Male\":1 }\n",
    "replacement_rule2 = {\"yes\":1, \"no\":0}\n",
    "replacement_rule3 = {\"Always\":0, \"Frequently\":1, \"Sometimes\":2, \"no\":3}\n",
    "replacement_rule4 = {\"Automobile\":0, \"Bike\":1, \"Motorbike\":2, \"Public_Transportation\":3, \"Walking\":4}\n",
    "for oneline in x_test_cat:\n",
    "    for i in range(len(oneline)):\n",
    "        if i == 0:\n",
    "            oneline[i] = replacement_rule1.get(oneline[i])\n",
    "        elif i in [1,2,4,5]:\n",
    "            oneline[i] = replacement_rule2.get(oneline[i])\n",
    "        elif i in [3,6]:\n",
    "            oneline[i] = replacement_rule3.get(oneline[i])\n",
    "        elif i == 7:\n",
    "            oneline[i] = replacement_rule4.get(oneline[i])\n",
    "            \n",
    "for oneline in x_train_cat:\n",
    "    for i in range(len(oneline)):\n",
    "        if i == 0:\n",
    "            oneline[i] = replacement_rule1.get(oneline[i])\n",
    "        elif i in [1,2,4,5]:\n",
    "            oneline[i] = replacement_rule2.get(oneline[i])\n",
    "        elif i in [3,6]:\n",
    "            oneline[i] = replacement_rule3.get(oneline[i])\n",
    "        elif i == 7:\n",
    "            oneline[i] = replacement_rule4.get(oneline[i])\n",
    "\n",
    "x_train_cat = np.array(x_train_cat).astype('int32')\n",
    "x_test_cat = np.array(x_test_cat).astype('int32')\n",
    "x_train_num = np.array(x_train_num).astype('float64')\n",
    "x_test_num = np.array(x_test_num).astype('float64')\n",
    "\n",
    "# Perform 1HOT conversion operation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc1 = OneHotEncoder()\n",
    "x_train_1hot = enc1.fit(x_train_cat)\n",
    "x_train_1hot = enc1.transform(x_train_cat).toarray()\n",
    "x_train_num_full = np.hstack((x_train_1hot,x_train_num))\n",
    "\n",
    "x_test_1hot = enc1.transform(x_test_cat).toarray()\n",
    "x_test_num_full = np.hstack((x_test_1hot,x_test_num))\n",
    "\n",
    "# discretized numeric features\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "enc2 = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy=\"uniform\")\n",
    "enc2.fit(x_train_num)\n",
    "\n",
    "x_train_disc = enc2.transform(x_train_num)\n",
    "x_train_cat_full = np.hstack((x_train_cat,x_train_disc))\n",
    "\n",
    "x_test_disc = enc2.transform(x_test_num)\n",
    "x_test_cat_full = np.hstack((x_test_cat,x_test_disc))\n",
    "          \n",
    "onehot_cat_name = enc1.get_feature_names(cat_features_name)\n",
    "x_cat_name = np.hstack((cat_features_name,num_features_name))\n",
    "x_num_name = np.hstack((onehot_cat_name,num_features_name))\n",
    "\n",
    "\n",
    "################################\n",
    "### Your code ends here ########\n",
    "################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train_cat[0])==8\n",
    "assert len(x_train_num[0])==8\n",
    "assert len(x_train_cat_full[0])==16\n",
    "assert len(x_train_num_full[0])==31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: A Weighted Random baseline [1.25 marks]\n",
    "\n",
    "Implement a <b>weighted random</b> baseline that randomly assigns a class to each test instance, weighting the class assignment according to <b>class prior probabilities</b>, as introduced in the Evaluation lecture and discuss your results.\n",
    "\n",
    "### 2-A: Implement a weighted random baseline [0.5 marks]\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "You will implement the baseline function\n",
    "1. `weight_random`\n",
    "<ul>\n",
    "  <li>input:\n",
    "      <ul>\n",
    "          <li> test labels</li>\n",
    "          <li> the prior probability of class \"obese\" </li>          \n",
    "      </ul>\n",
    "  </li>\n",
    "  <li>output: error rate on the test (fraction of incorrect predictions)</li>          \n",
    "</ul> \n",
    "    \n",
    "\n",
    "<b> You should implement this function from scratch yourself. We have already imported the two functions that we assume you will need. Do not import any other function or library.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 199.0 obese and 224.0 non-obese test instances\n",
      "The prior probability of obese is 0.46\n",
      "The weighted random baseline's error is: 0.51\n",
      "the mean value of 1000 times : 0.4965981087470449\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from random import random\n",
    "\n",
    "test_obese = y_test.count(\"obese\")/len(y_test) ## calculate the fraction of obese patients in the test set \n",
    "prior_obese = y_train.count(\"obese\")/len(y_train) ## calculate the prior probability of class obese in the train set\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "def weight_random(y_test,prior_obese):\n",
    "    error = 0    \n",
    "    \n",
    "    ## insert your code here\n",
    "    label = []\n",
    "    n = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if random() > prior_obese:\n",
    "            label.append(\"notObese\")\n",
    "        else:\n",
    "            label.append(\"obese\")\n",
    "    for i in range(len(label)):\n",
    "        if label[i] != y_test[i]:\n",
    "            n = n+1\n",
    "    error = n/len(y_test)\n",
    "    return error \n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "err = weight_random(y_test,prior_obese)\n",
    "print(\"There are\",test_obese*len(y_test),\"obese and\",(1-test_obese)*len(y_test),\"non-obese test instances\")\n",
    "print(\"The prior probability of obese is\", round(prior_obese,2))\n",
    "print(\"The weighted random baseline's error is:\", round(err,2))\n",
    "\n",
    "\n",
    "##########################################\n",
    "## insert code for for Question 2-B here:\n",
    "##########################################\n",
    "errors = 0\n",
    "for i in range(1000):\n",
    "    errors = errors + weight_random(y_test,prior_obese)\n",
    "errors_mean = errors/1000\n",
    "print(\"the mean value of 1000 times :\", errors_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-B: Weighted Random baseline Discussion [0.75 marks]\n",
    "\n",
    "Repeat the random baseline 1000 times, what would be the <b>average error rate</b> of your weighted random baseline on the test dataset? You should update your code above in order to achieve this. Please indicate through comments which parts of the code are relevant to question 2B.\n",
    "\n",
    "What would the error rate converge to (write a formula based on the prior probability of obese, `prior_obese`, and the fraction of obese samples in the test data, `test_obese`)?\n",
    "\n",
    "Please limit your answer to 2-3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2-B)\n",
    "The average error rate is 0.4982411347517732.\n",
    "Error rate converge to test_obese * (1 - prior_obese) + (1-test_obese) * prior_obese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: One-R baseline [2.5 marks]\n",
    "\n",
    "In this question you will implement the one-R baseline using a helper function and discuss your results. Please refer to the Evaluation lecture for One-R baseline. \n",
    "\n",
    "### Question 3-A: Implement a helper function [0.75 mark]\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "You will implement a function that calculates the error rate for each attribute if we select that attribute as the one rule for classification.\n",
    "1. `feature_error`\n",
    "<ul>\n",
    "  <li>input:\n",
    "      <ul>\n",
    "          <li> features</li>\n",
    "          <li> labels</li>\n",
    "      </ul>\n",
    "  </li>\n",
    "  <li>output:\n",
    "      <ul>\n",
    "          <li> a dictionary of the form {feature: error rate on the train} where each \n",
    "        feature is denoted by its ID (position in the train dataset)</li>\n",
    "          <li> a dictionary of the form {feature value: majority class label} where the majority class label is either \"obese\" or \"notObese\" and feature values are the best feature's possible values (feature with smallest error rate)</li>\n",
    "      </ul>\n",
    "    </li>   \n",
    "</ul> \n",
    "\n",
    "<b> You should implement this function from scratch yourself. You may use native Python libraries like math, numpy, collections to help you, but you may not use existing implementations.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "def feature_error(features, labels):\n",
    "    feat_error = {}  \n",
    "    feat_rule = {}\n",
    "    \n",
    "    ## insert your code here\n",
    "    if len(features) != len(labels):\n",
    "        print(\"failly match\")\n",
    "        return\n",
    "    \n",
    "    import numpy as np\n",
    "    from collections import Counter\n",
    "    features = np.array(features).T\n",
    "    \n",
    "    correct_most = 0\n",
    "    for i in range(len(features)):\n",
    "        feature_set = set(features[i])\n",
    "        \n",
    "        rule_temp = {}\n",
    "        correct_time = 0\n",
    "        for feature in feature_set:\n",
    "            one_feature_list = []\n",
    "            \n",
    "            for j in range(len(features[i])):\n",
    "                if features[i][j] == feature:\n",
    "                    one_feature_list.append((features[i][j], labels[j]))\n",
    "                    \n",
    "            featureAndlabel = Counter(one_feature_list).most_common(1)[0][0]\n",
    "            rule_temp[featureAndlabel[0]] = featureAndlabel[1]\n",
    "            correct_time += Counter(one_feature_list).most_common(1)[0][1]\n",
    "        \n",
    "        if correct_most < correct_time:\n",
    "            feat_rule = rule_temp\n",
    "            correct_most = correct_time\n",
    "        \n",
    "        feat_error[i] = 1 - correct_time / len(labels)\n",
    "\n",
    "    return feat_error, feat_rule\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_features = [[2,2], [2,1], [4,2], [4,1]]\n",
    "sample_labels = [\"yes\",\"yes\",\"no\",\"no\"]\n",
    "sample_error, sample_rule= feature_error(sample_features, sample_labels)\n",
    "\n",
    "# where index 0 refers to \"feature 1\" and index 1 to \"feature 2\"\n",
    "assert sample_error[0]==0.0\n",
    "assert sample_error[1]==0.5\n",
    "\n",
    "# where 2 and 4 refer to the values of the best feature \n",
    "# and \"yes\",\"no\" to the majority class label\n",
    "assert sample_rule == {2:\"yes\",4:\"no\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3-B: Implement One-R baseline [0.75 mark]\n",
    "\n",
    "- Apply your `feature_error` function to the full training data *x_train_cat_full*, and determine the (i) feature with *highest* error rate and (ii) feature with *lowest* error rate. Store the name (string) of the corresponding highest/lowest error rate features in `highest_err_feature_name` and `lowest_err_feature_name`, respectively.\n",
    "\n",
    "- The feature with *lowest* error will constitute your 1-R predictor, which you should use to predict the class labels for the *x_test_cat_full* (`one_r_predictions`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notObese', 'obese', 'notObese', 'notObese', 'notObese', 'notObese', 'obese', 'notObese', 'notObese', 'notObese']\n",
      "The feature with highest error is: gender\n",
      "The feature with lowest error is: weight\n",
      "The all codes are shown in 3_C part.\n"
     ]
    }
   ],
   "source": [
    "one_r_predictions = []\n",
    "\n",
    "highest_err_feature_name = \"\" # feature name with highest error\n",
    "lowest_err_feature_name = \"\" # feature name with lowest error\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "features_error, features_rule = feature_error(x_train_cat_full, y_train)\n",
    "\n",
    "lowest_error = 1\n",
    "highest_error = 0\n",
    "lowest_error_index = -1\n",
    "for key, value in features_error.items():\n",
    "    if value > highest_error:\n",
    "        highest_error = value\n",
    "        highest_err_feature_name = x_cat_name[key]\n",
    "        \n",
    "    if value < lowest_error:\n",
    "        lowest_error = value\n",
    "        lowest_err_feature_name = x_cat_name[key]\n",
    "        lowest_error_index = key\n",
    "\n",
    "for one_line in x_test_cat_full:\n",
    "    one_r_predictions.append(features_rule[one_line[lowest_error_index]])\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(one_r_predictions[:10])\n",
    "print(f\"The feature with highest error is: {highest_err_feature_name}\")\n",
    "print(f\"The feature with lowest error is: {lowest_err_feature_name}\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "## insert code for for Question 3-D here:\n",
    "##########################################\n",
    "print(\"The all codes are shown in 3_C part.\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3-C: Compute Mutual Information (MI) [0.5 mark]\n",
    "\n",
    "You will calculate the mutual information between your features  *x_train_cat_full* and the class label and determine the (i) feature with *highest* MI and (ii) feature with *lowest* MI. Store the name (string) of the corresponding highest/lowest MI features in `highest_mi_feature_name` and `lowest_mi_feature_name`, respectively.\n",
    "\n",
    "**Note:** You may use classes and functions from ```scikit-learn``` but make sure that you specify your dataset is categorical (discrete) otherwise it will be assumed that your data is numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature with highest MI is: weight\n",
      "The feature with lowest MI is: gender\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation is: -0.9042499433493082\n"
     ]
    }
   ],
   "source": [
    "highest_mi_feature_name = \"\" # feature with highest MI\n",
    "lowest_mi_feature_name = \"\" # feature with lowest MI\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "all_feature_mi = mutual_info_classif(x_train_cat_full, y_train, discrete_features=True)\n",
    "\n",
    "all_feature_mi = all_feature_mi.tolist()\n",
    "max_mutual_info = max(all_feature_mi)\n",
    "min_mutual_info = min(all_feature_mi)\n",
    "\n",
    "max_mutual_index = all_feature_mi.index(max_mutual_info)\n",
    "min_mutual_index = all_feature_mi.index(min_mutual_info)\n",
    "\n",
    "highest_mi_feature_name = x_cat_name[max_mutual_index]\n",
    "lowest_mi_feature_name = x_cat_name[min_mutual_index]\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"The feature with highest MI is: {highest_mi_feature_name}\")\n",
    "print(f\"The feature with lowest MI is: {lowest_mi_feature_name}\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "## insert code for for Question 3-D here:\n",
    "##########################################\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(all_feature_mi, features_error.values(), 'ro')\n",
    "plt.xlabel('MI values')\n",
    "plt.ylabel('Error values')\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "print(\"The correlation is:\", np.corrcoef(all_feature_mi, list(features_error.values()))[1][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3-D: Discussion of One-R and Mutual Information [0.5 mark]\n",
    "In addition to features leading to lowest/highest error rates and MI, inspect the MI and error rates for all 16 features. You should update your code above in order to achieve this. Please indicate through comments which parts of the code are relevant to question 3D.  \n",
    "\n",
    "Are you finding different or somewhat similar patterns in the results? Explain the difference and similarity between MI and error rate formula in One-R?\n",
    "\n",
    "Please limit your answer to 2-3 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-D)  \n",
    "The correlation is: -0.9042499433493082. There is strong negative correlation between MI and error rate.\n",
    "According to the formula of error rate and MI, we find that error rate only considers the overall error situation, but MI is used to measure the correlation between label and feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Naive Bayes [2 marks]\n",
    "\n",
    "We will construct two Naive Bayes classifiers\n",
    "\n",
    "1. One for instances with categorical attributes.\n",
    "2. One for instances with numerical attributes.\n",
    "\n",
    "\n",
    "For each classifier, you will \n",
    "1. Train it on the training set\n",
    "2. Use the models to predict labels for the test set\n",
    "\n",
    "### Question 4-A: Implementing the Naive Bayes classifiers [0.5 marks]\n",
    "\n",
    "Implement two functions which train a NB classifier given the specified input feature types and predict labels for a given test set.\n",
    "\n",
    "You may add additional quantities to your `return` statements.\n",
    "\n",
    "<b> You may (and are, indeed, encouraged) to, use the existing NB implementations from `sklearn`. You should use the default parameterizations of these algorithms.</b> \n",
    "    \n",
    "**Note:** If you choose to implement your classifiers from scratch, please use Laplace smoothing (alpha=1) for the categorical feature NB.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## your code begins here\n",
    "##################################\n",
    "\n",
    "def nb_cat_features(train_features, train_labels, test_features):\n",
    "    predictions = []      \n",
    "    \n",
    "    ## insert your code here\n",
    "    from sklearn.naive_bayes import CategoricalNB\n",
    "    NB_Cate = CategoricalNB()\n",
    "    NB_Cate.fit(train_features, train_labels)\n",
    "    predictions = NB_Cate.predict(test_features)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "def nb_num_features(train_features, train_labels, test_features):\n",
    "    predictions = []\n",
    "        \n",
    "    ## insert your code here\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    NB_Gau = GaussianNB()\n",
    "    NB_Gau.fit(train_features, train_labels)\n",
    "    predictions = NB_Gau.predict(test_features)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4-B: Apply your classifiers to the data sets you created in Questions 1 [0.5 marks]\n",
    "\n",
    "...namely, the\n",
    "\n",
    "1. categorical features `x_{train,test}_cat`\n",
    "2. numerical features `x_{train,test}_num`\n",
    "3. combined discretized numerical and categorical features `x_{train,test}_cat_full`\n",
    "4. combined 1-hot categorical features and numerical `x_{train,test}_num_full` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical NB predicted class distribution \t Counter({'notObese': 237, 'obese': 186})\n",
      "Categorical NB predicted class distribution\t Counter({'obese': 261, 'notObese': 162})\n",
      "Full Categorical NB predicted class distribution\t Counter({'notObese': 215, 'obese': 208})\n",
      "Full Numerical NB predicted class distribution\t Counter({'obese': 261, 'notObese': 162})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "cat_nb_predictions = nb_cat_features(x_train_cat, y_train, x_test_cat) # predictions for x_test_cat\n",
    "num_nb_predictions = nb_num_features(x_train_num, y_train, x_test_num) # predictions for x_test_num\n",
    "full_cat_nb_predictions = nb_cat_features(x_train_cat_full, y_train, x_test_cat_full) # predictions for x_test_cat_full\n",
    "full_num_nb_predictions = nb_num_features(x_train_num_full, y_train, x_test_num_full) # predictions for x_test_num_full\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"Numerical NB predicted class distribution \\t {Counter(num_nb_predictions)}\")\n",
    "print(f\"Categorical NB predicted class distribution\\t {Counter(cat_nb_predictions)}\")\n",
    "print(f\"Full Categorical NB predicted class distribution\\t {Counter(full_cat_nb_predictions)}\")\n",
    "print(f\"Full Numerical NB predicted class distribution\\t {Counter(full_num_nb_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4-C: Combining NB predictions [1 mark]\n",
    "\n",
    "You are asked to construct another Naive Bayes classifier for instances with the full set of numerical *and* categorical attributes (no discretization and no one-hot encoding), ensuring that your classifier computes posterior class probabilities $p(y|x)$ as $p(x|y)p(y)$. Explain such Naive Bayes classifier implementation on the full attribute set (numerical and categorical features) \n",
    "\n",
    "Please limit your answer to 3-4 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4-C)\n",
    "A full set is divided into two parts, numerical attributes and categorical attributes. Numerical attributes and Categorical attributes use GaussianNB and Categorical to predict their corresponding posterior class probabilities respectively. Multiply the two obtained posterior classes probabilities to obtain full probabilities. Find the label with the highest probability and get the prediction result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8534278959810875"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nb_all(x_train_num, x_train_cat, x_test_num, x_test_cat, y_train):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    num_NB = GaussianNB()\n",
    "    num_NB.fit(x_train_num, y_train)\n",
    "    num_prob = num_NB.predict_proba(x_test_num)\n",
    "\n",
    "    from sklearn.naive_bayes import CategoricalNB\n",
    "    cat_NB = CategoricalNB()\n",
    "    cat_NB.fit(x_train_cat, y_train)\n",
    "    cat_prob = cat_NB.predict_proba(x_test_cat)\n",
    "\n",
    "    full_prob = cat_prob * num_prob\n",
    "    prediction = []\n",
    "    for oneLine in full_prob:\n",
    "        if oneLine[0] > oneLine[1]:\n",
    "            prediction.append('notObese')\n",
    "        else:\n",
    "            prediction.append('obese')\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "prediction = nb_all(x_train_num, x_train_cat, x_test_num, x_test_cat, y_train)\n",
    "accuracy_score(y_test, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Logistic Regression [1.25 marks]\n",
    "\n",
    "1. First find the mean and standard deviation of values for each feature in `x_train_num_full` and then using the calculated mean and standard deviation values, standardize `x_{train,test}_num_full` such that each feature has mean=0 and std= 1. Store the standardized data to `x_{train,test}_norm_full`.\n",
    "\n",
    "2. Implement a Logistic Regression classifier, and apply it to the standardized full numeric training data set (`x_{train,test}_norm_full`).\n",
    "\n",
    "<b> You should use existing implementation in sklearn with default parameters for Logistic Regression. You may use classes and functions from ```scikit-learn``` for standardization.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-A: The Logistic Regression classifier [0.5 marks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR predicted class distribution\t Counter({'notObese': 227, 'obese': 196})\n",
      "\n",
      "Feature Name, 1st coef: weight\n",
      "weight: 6.407399558263892\n",
      "Feature Name, 1st MI: weight\n",
      "\n",
      "Feature Name, 2nd coef: height\n",
      "weight: -1.6757072775038981\n",
      "Feature Name, 2nd MI: family_history\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_predictions = []\n",
    "x_train_norm_full = []\n",
    "x_test_norm_full = []\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "\n",
    "SS.fit(x_train_num_full)\n",
    "x_train_norm_full = SS.transform(x_train_num_full)\n",
    "x_test_norm_full = SS.transform(x_test_num_full)\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train_norm_full, y_train)\n",
    "lr_predictions = LR.predict(x_test_norm_full)\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "print(f\"LR predicted class distribution\\t {Counter(lr_predictions)}\")\n",
    "\n",
    "\n",
    "##########################################\n",
    "## insert code for for Question 5-B here:\n",
    "##########################################\n",
    "highest_coef = sorted(LR.coef_[0].tolist(), key = abs, reverse=True)[0]\n",
    "second_highest_coef = sorted(LR.coef_[0].tolist(), key = abs, reverse=True)[1]\n",
    "\n",
    "highest_coef_i = LR.coef_[0].tolist().index(highest_coef)\n",
    "second_highest_coef_i = LR.coef_[0].tolist().index(second_highest_coef)\n",
    "\n",
    "highest_coef_name = x_num_name[highest_coef_i]\n",
    "second_highest_coef_name = x_num_name[second_highest_coef_i]\n",
    "\n",
    "print()\n",
    "print(\"Feature Name, 1st coef:\", highest_coef_name)\n",
    "print(highest_coef_name + \":\", highest_coef)\n",
    "print(\"Feature Name, 1st MI:\", highest_mi_feature_name)\n",
    "print()\n",
    "\n",
    "second_highest_mi_index = all_feature_mi.index(sorted(all_feature_mi, reverse=True)[1])\n",
    "second_highest_mi_feature_name = x_cat_name[second_highest_mi_index]\n",
    "print(\"Feature Name, 2nd coef:\", second_highest_coef_name)\n",
    "print(highest_coef_name + \":\", second_highest_coef)\n",
    "print(\"Feature Name, 2nd MI:\", second_highest_mi_feature_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5-B: Inspect the learned coefficients [0.75 marks]\n",
    "\n",
    "**Part 1**\n",
    "Inspect the coefficients (weights) learned by the classifier, and compare them to the features with the highest MI values in Question 3. You should update your code above in order to achieve this. Please indicate through comments which parts of the code are relevant to question 5B.\n",
    "\n",
    "**Part 2**\n",
    "What does a high coefficient for a feature imply? Report the two feature names for which your model learns the most important coefficients? Compare your answer to the highest MI features you detected in Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Q5-B) \n",
    "The higher the coefficient is, the greater the influence of feature on label is.\n",
    "The two features with the largest coefficient are weight and height.\n",
    "The two features with the largest MI are weight and family_history.\n",
    "The feature with the largest coefficient and MI is weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Evaluation [1 mark]\n",
    "\n",
    "We will evaluate our baselines and classifiers on the instances in the test set. \n",
    "\n",
    "Compute \n",
    "- error rate\n",
    "- macro-averaged F1 score \n",
    "\n",
    "for the One-R baselines, the four NB classifiers (on 4 datasets) and the LR classifier.\n",
    "\n",
    "**You may use existing implementations and/or Python libraries like numpy, scipy or sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One  R\t\tError: 0.23\tMacro F1: 0.74\n",
      "NB Num \t\tError: 0.13\tMacro F1: 0.87\n",
      "NB Cat \t\tError: 0.21\tMacro F1: 0.79\n",
      "NB Num Full \tError: 0.21\tMacro F1: 0.79\n",
      "NB Cat Full \tError: 0.19\tMacro F1: 0.81\n",
      "LR \t\tError: 0.06\tMacro F1: 0.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "one_r_err = 0\n",
    "one_r_f1 = 0\n",
    "\n",
    "\n",
    "num_nb_err = 0\n",
    "num_nb_f1 = 0\n",
    "\n",
    "\n",
    "cat_nb_err = 0\n",
    "cat_nb_f1 = 0\n",
    "\n",
    "\n",
    "full_cat_nb_err = 0\n",
    "full_cat_nb_f1 = 0\n",
    "\n",
    "\n",
    "full_num_nb_err = 0\n",
    "full_num_nb_f1 = 0\n",
    "\n",
    "lr_err = 0\n",
    "lr_f1 = 0\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "one_r_f1 = f1_score(y_test, one_r_predictions, average=\"macro\")\n",
    "num_nb_f1 = f1_score(y_test, num_nb_predictions, average=\"macro\")\n",
    "cat_nb_f1 = f1_score(y_test, cat_nb_predictions, average=\"macro\")\n",
    "full_cat_nb_f1 = f1_score(y_test, full_cat_nb_predictions, average=\"macro\")\n",
    "full_num_nb_f1 = f1_score(y_test, full_num_nb_predictions, average=\"macro\")\n",
    "lr_f1 = f1_score(y_test, lr_predictions, average=\"macro\")\n",
    "\n",
    "one_r_accuracy = accuracy_score(y_test, one_r_predictions)\n",
    "one_r_err = 1 - one_r_acc\n",
    "\n",
    "num_nb_accuracy = accuracy_score(y_test, num_nb_predictions)\n",
    "num_nb_err = 1 - num_nb_acc\n",
    "\n",
    "cat_nb_accuracy = accuracy_score(y_test, cat_nb_predictions)\n",
    "cat_nb_err = 1 - cat_nb_acc\n",
    "\n",
    "full_cat_nb_accuracy = accuracy_score(y_test, full_cat_nb_predictions)\n",
    "full_cat_nb_err = 1 - full_cat_nb_acc\n",
    "\n",
    "full_num_nb_accuracy = accuracy_score(y_test, full_num_nb_predictions)\n",
    "full_num_nb_err = 1 - full_num_nb_acc\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "lr_err = 1 - lr_acc\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "\n",
    "print(f\"One  R\\t\\tError: {round(one_r_err, 2)}\\tMacro F1: {round(one_r_f1, 2)}\")\n",
    "print(f\"NB Num \\t\\tError: {round(num_nb_err, 2)}\\tMacro F1: {round(num_nb_f1, 2)}\")\n",
    "print(f\"NB Cat \\t\\tError: {round(cat_nb_err, 2)}\\tMacro F1: {round(cat_nb_f1, 2)}\")\n",
    "print(f\"NB Num Full \\tError: {round(full_num_nb_err, 2)}\\tMacro F1: {round(full_num_nb_f1, 2)}\")\n",
    "print(f\"NB Cat Full \\tError: {round(full_cat_nb_err, 2)}\\tMacro F1: {round(full_cat_nb_f1, 2)}\")\n",
    "print(f\"LR \\t\\tError: {round(lr_err, 2)}\\tMacro F1: {round(lr_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Discussion [5 marks] \n",
    "\n",
    "Critically analyze the performance of the models by answering the following questions. \n",
    "\n",
    "**(a)** The four Naive Bayes (NB) classifiers lead to different performance. Which of the four NB classifiers performs best, and why do you think it is the case? **[1 mark]**\n",
    "\n",
    "**(b)** You used unstandardized data in your numeric Naive Bayes, what would be the effect of standardization on the results of numeric Naive Bayes? Why? **[0.75 mark]**\n",
    "\n",
    "**(c)** You chose to use equal-width binning for your continuous features, discuss the drawbacks of considering this approach with a fixed bin number. Would you select a different binning strategy, and if so which? **[1 mark]**\n",
    "\n",
    "**(d)** Assume that your best classifier will be deployed in a hospital to warn new patients if they are prone to obesity. You want to avoid falsely diagnosing a patient as obese. Can you reassure the hospital with the evaluation results in Q7 that your classifier is adequate? If not, describe an alternative evaluation metric and explain why it is more appropriate.**[0.75 mark]**\n",
    "\n",
    "**(e)** Do you observe a clear difference in performance between NB and Logistic Regression (LR) in Q7? Referring to the number of parameters and assumptions underlying each model, provide one reason why NB might outperform LR, and one reason why LR might outperform NB. In your answer, refer back to the *Obesity* data set used in this assignment. **[1 mark]**\n",
    "\n",
    "**(f)** Assume that you have access to a (hypothetical) infinitely large database (i.e., an enormous version of the Obesity data set used in this assignment). Is it true that Logistic Regression will achieve perfect test set performance? Why (not)? **[0.5 mark]**\n",
    "\n",
    "\n",
    "<b>We expect a maximum of 2-3 sentences per question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Numeric NB performs best. Compared with numeric NB, other NB classiers have more features and have learned more noisy than useful information. It's difficult for NB to learn too many features, beacuse it's a simple machine learning model.\n",
    "\n",
    "\n",
    "(b)There will be no impact. Standardization can adjust features' values in different ranges to 0-1. NB assumes that features are independent of each other, so the standardized features will not affect NB.\n",
    "\n",
    "\n",
    "(c) The disadvantage of equal-width binning is that it is an unbanlanced bin and easy to be effected by extreme value. I prefer to use \"kmeans\" strategy which values in each bin have the same nearest center of a 1D k-means cluster.\n",
    "\n",
    "\n",
    "(d)It is not adequate. I prefer to use Precision which function is (TP/ (TP + FP)). In order to avoid falsely diagnosing a patient as obese,we want to make it more likely that people who are considered obese are actually obese, and that's the definition of accuracy.\n",
    "\n",
    "\n",
    "(e)Yes,it is clear different.\n",
    "\n",
    "When there is correlation between instance features, LR is better than NB because there is no need to make independence assumption in LR. In our data, many features are correlated, such as height and weight.\n",
    "\n",
    "When instance has many features, NB will be better than LR, because LR has a strong learning ability and may learn a lot of useless noise, resulting in overfitting. Our data now has 16 features, but as our features become more numerous, using logistic regression may result in overfitting.\n",
    "\n",
    "\n",
    "(f)It may not perfect. LR is linear classifier because its decision boundary is linear. It is rarely to find data separate linearly in real word. The more data, the more outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Yuqing Zhou\n",
    "   \n",
    "   <b>Dated</b>: 2021.9.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
